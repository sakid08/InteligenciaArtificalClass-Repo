{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# --- CONFIGURACI√ìN CR√çTICA ---\n",
    "# PEGA AQU√ç LA RUTA QUE TE SALI√ì EN EL PASO 1 (Sin el salto de l√≠nea final)\n",
    "# Ejemplo: \"/home/luisma/.cache/huggingface/...\"\n",
    "\n",
    "MODELO_LOCAL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "\n",
    "DATASET_FILE = \"./dataset_final.jsonl\"\n",
    "OUTPUT_DIR = \"tutor_algoritmos_v1\" # Aqu√≠ se guardar√° tu IA\n",
    "\n",
    "# Configuraci√≥n de memoria\n",
    "max_seq_length = 2048 \n",
    "load_in_4bit = True \n",
    "\n",
    "# 1. CARGAR MODELO DESDE DISCO (OFFLINE)\n",
    "print(f\"‚è≥ Cargando modelo base desde: {MODELO_LOCAL}\")\n",
    "if not os.path.exists(MODELO_LOCAL):\n",
    "    raise FileNotFoundError(f\"‚ùå ¬°ERROR! No encuentro la carpeta: {MODELO_LOCAL}\\nVerifica la ruta en el Paso 1.\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODELO_LOCAL, # <--- Aqu√≠ usamos la ruta local\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 2. CONFIGURAR ADAPTADORES (LoRA)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 3. CARGAR TU DATASET\n",
    "print(f\"üìÇ Cargando datos de: {DATASET_FILE}\")\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_FILE, split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# 4. ENTRENAR (MODO PRODUCCI√ìN)\n",
    "print(\"üí™ Iniciando Entrenamiento con RTX 5070...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, \n",
    "        gradient_accumulation_steps = 4, \n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # 1 vuelta completa a los 2300 datos\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(), \n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"checkpoints\", # Temporales\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"üéâ ¬°Entrenamiento finalizado exitosamente!\")\n",
    "\n",
    "# 5. EXPORTAR A GGUF (Para Ollama)\n",
    "print(f\"üíæ Guardando tu modelo final en carpeta '{OUTPUT_DIR}'...\")\n",
    "model.save_pretrained_gguf(OUTPUT_DIR, tokenizer, quantization_method = \"q8_0\")\n",
    "\n",
    "print(\"‚úÖ ¬°LISTO! Todo ha terminado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "# Tu checkpoint\n",
    "ADAPTADORES = \"checkpoints/checkpoint-295\" \n",
    "CARPETA_RAW = \"tutor_lora_raw\"\n",
    "\n",
    "print(\"üî• Cargando checkpoint...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = ADAPTADORES,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, \n",
    ")\n",
    "\n",
    "print(f\"‚ö° Guardando archivos crudos en '{CARPETA_RAW}'...\")\n",
    "# Guardamos solo el adaptador en formato HuggingFace est√°ndar\n",
    "model.save_pretrained(CARPETA_RAW)\n",
    "tokenizer.save_pretrained(CARPETA_RAW)\n",
    "\n",
    "print(\"‚úÖ ¬°Listo! Paso 1 completado.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
